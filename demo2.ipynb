{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b40d341b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/ossowski/anaconda3/envs/llava/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 118287/118287 [00:32<00:00, 3675.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped over 363 bad segmentations (no pixels)\n",
      "Loaded 671067 examples from retrieval/object_classification/retrieval_set_1000000_cropped_dinov2-large.pkl.pkl\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from models.visionllama import VisionLLaMA\n",
    "from dataset.objectCOCO import COCOObjectDataset\n",
    "\n",
    "\n",
    "with open(\"configs/config.yaml\", 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "config['use_retrieval'] = True\n",
    "dataset = COCOObjectDataset(config, split=\"train\", patch_size=config['patch_size'], max_examples_per_class = config[\"examples_per_class\"])\n",
    "model = None\n",
    "old_config = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f321b9a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fc98f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import os\n",
    "import skimage\n",
    "import math\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "def _get_ViT_mask(mask, height, width, output_height, output_width):\n",
    "        \n",
    "    \n",
    "    pooled_mask = skimage.measure.block_reduce(mask, block_size=(math.floor(height / 16), math.floor(width / 16)), func=np.max)\n",
    "\n",
    "    result_height, result_width = pooled_mask.shape\n",
    "    # If the result is smaller than 16x16, pad it with zeros\n",
    "    if result_height < output_height or result_width < output_width:\n",
    "        pad_height = output_height - result_height\n",
    "        pad_width = output_width - result_width\n",
    "        pooled_mask = np.pad(pooled_mask, ((0, pad_height), (0, pad_width)), mode='constant')\n",
    "\n",
    "    if result_height > output_height or result_width > output_width:\n",
    "        pooled_mask = pooled_mask[:output_height, :output_width]\n",
    "\n",
    "    assert pooled_mask.shape == (output_height,output_width)\n",
    "    return torch.BoolTensor(np.append(1, pooled_mask.flatten()))\n",
    "\n",
    "def sleep(im):\n",
    "    time.sleep(2)\n",
    "    ret = [im[\"background\"]]\n",
    "    for layer in im[\"layers\"]:\n",
    "        ret.append(layer)\n",
    "    return ret\n",
    "    #return [im[\"background\"], im[\"layers\"][0], im[\"layers\"][1], im['composite']]\n",
    "\n",
    "def show_anns(anns, ax):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
    "    ax.set_autoscale_on(False)\n",
    "\n",
    "    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n",
    "    img[:,:,3] = 0\n",
    "    for ann in sorted_anns:\n",
    "        m = ann['segmentation']\n",
    "        color_mask = np.concatenate([np.random.random(3), [0.35]])\n",
    "        img[m] = color_mask\n",
    "    ax.imshow(img)\n",
    "\n",
    "def generate_predictions(question, images, task, backbone, use_retrieval, use_object_encoder_checkpoint, freeze_llm, chat_history):\n",
    "    global model\n",
    "    global old_config\n",
    "    image = Image.open(images.root[0].image.path).convert('RGB')\n",
    "    segmentations = [Image.open(x.image.path).convert('RGB') for x in images.root[1:]]\n",
    "    \n",
    "    config['freeze_llm'] = freeze_llm\n",
    "    config['llm_model'] = backbone\n",
    "    config['task'] = task\n",
    "\n",
    "    if not use_object_encoder_checkpoint:\n",
    "        config['pretrained_object_encoder_checkpoint'] = \"None\"\n",
    "    else:\n",
    "        config['pretrained_object_encoder_checkpoint'] = \"./llama_2_7b_adapter\"\n",
    "\n",
    "    config['use_retrieval'] = use_retrieval\n",
    "\n",
    "    if \"llama\" in backbone:\n",
    "        output_width, output_height = 16, 16\n",
    "        \n",
    "    elif \"llava\" in backbone:\n",
    "        output_width, output_height = 24, 24\n",
    "\n",
    "    config['patch_size'] = output_width\n",
    "\n",
    "\n",
    "    if old_config != config:\n",
    "        if config['use_retrieval']:\n",
    "            model = VisionLLaMA(config, retrieval_fn = lambda x, y: dataset.retrieve_closest(x, config[\"retrieval_k\"], b_num = y))    \n",
    "\n",
    "        else:\n",
    "            model = VisionLLaMA(config)\n",
    "        model.load()\n",
    "        model.eval()\n",
    "        old_config = config.copy()\n",
    "\n",
    "    seg_width, seg_height = image.size\n",
    "    \n",
    "    vit_masks = []\n",
    "\n",
    "    \n",
    "    cropped_images = []\n",
    "    for segmentation in segmentations:\n",
    "        seg = np.array(segmentation)\n",
    "        if np.sum(seg, axis = None) == 0:\n",
    "            continue\n",
    "        else:\n",
    "        \n",
    "            mask = np.any(seg != [0, 0, 0], axis=-1)\n",
    "\n",
    "            if config[\"crop_image\"]:\n",
    "                # print(mask.shape)\n",
    "                # print(img.shape)\n",
    "                img = np.array(image)\n",
    "                img[~mask] = np.array([255,255,255])\n",
    "                \n",
    "                \n",
    "                # Find the indices of non-zero elements in the binary mask\n",
    "                non_zero_indices = np.where(mask)\n",
    "\n",
    "                # Get the minimum and maximum values along each axis\n",
    "                min_x, min_y = np.min(non_zero_indices[1]), np.min(non_zero_indices[0])\n",
    "                max_x, max_y = np.max(non_zero_indices[1]), np.max(non_zero_indices[0])\n",
    "\n",
    "                #img = img[min_x: max_x, min_y: max_y]\n",
    "                img = img[min_y: max_y, min_x: max_x]\n",
    "\n",
    "                cropped_image = Image.fromarray(np.uint8(img)).convert('RGB')\n",
    "                cropped_images.append(cropped_image)\n",
    "                print(min_x, min_y, max_x, max_y)\n",
    "            \n",
    "            vit_masks.append(_get_ViT_mask(mask, seg_height, seg_width, output_height, output_width))\n",
    "       \n",
    "       #print(_get_ViT_mask(mask, seg_height, seg_width).shape)\n",
    "    if len(vit_masks) > 0:\n",
    "        vit_masks = torch.stack(vit_masks, axis = 0)\n",
    "    imgs = [image] * len(vit_masks) if len(vit_masks) > 0 else [image]\n",
    " \n",
    "\n",
    "    prompts = None\n",
    "    masks = None\n",
    "    images = None\n",
    "    if config['use_retrieval']:\n",
    "        output, prompts, masks, images = model.generate(vit_masks, imgs, [question], return_retrieved_info=True, cropped_images = cropped_images)\n",
    "        chat_history.append((question, output))\n",
    "        retrieval_images = [Image.open(images[0][x]) for x in range(len(images[0]))]\n",
    "        retrieval_images += [cropped_image]\n",
    "        return chat_history, retrieval_images\n",
    "        \n",
    "    else:\n",
    "        output = model.generate(vit_masks, imgs, [question])\n",
    "        chat_history.append((question, output))\n",
    "        return chat_history, None\n",
    "    \n",
    "    print(prompts, masks, images)\n",
    "    # fig, ax = plt.subplots()\n",
    "    # ax.imshow(image)\n",
    "    # show_anns(masks, ax)\n",
    "    # plt.axis('off')\n",
    "\n",
    "    # img_buf = io.BytesIO()\n",
    "    # plt.savefig(img_buf, format='png')\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "with gr.Blocks(title=\"Olive\", theme=gr.themes.Base()).queue() as demo:\n",
    "    \n",
    "    with gr.Row():\n",
    "        \n",
    "        with gr.Column():\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    \n",
    "                    im = gr.ImageEditor(\n",
    "                        type=\"pil\"\n",
    "                        \n",
    "                    )\n",
    "                    with gr.Row():\n",
    "                        gallery = gr.Gallery(\n",
    "                            label=\"Segmentations\", show_label=False, elem_id=\"gallery\"\n",
    "                        , columns=[3], rows=[1], object_fit=\"contain\", height=200)\n",
    "\n",
    "\n",
    "                        \n",
    "                        # im_out_1 = gr.Image(type=\"pil\")\n",
    "                        # im_out_2 = gr.Image(type=\"pil\")\n",
    "                        # im_out_3 = gr.Image(type=\"pil\")\n",
    "                        # im_out_4 = gr.Image(type=\"pil\")\n",
    "                with gr.Column():\n",
    "                    chatbot = gr.Chatbot(elem_id=\"chatbot\", label=\"LLaVA Chatbot\", height=550)\n",
    "                    with gr.Row():\n",
    "                        \n",
    "                        with gr.Column(scale=8):\n",
    "                            textbox = gr.Textbox(show_label=False, placeholder=\"Enter text and press ENTER\", container=False)\n",
    "                        with gr.Column(scale=1, min_width=50):\n",
    "                            submit_btn = gr.Button(value=\"Send\", variant=\"primary\")\n",
    "                    retrieval_gallery = gr.Gallery(\n",
    "                            label=\"Segmentations\", show_label=False, elem_id=\"gallery\"\n",
    "                        , columns=[5], rows=[1], object_fit=\"contain\", height=\"auto\")\n",
    "                    task = gr.Dropdown([\"object_classification\", \"relation_prediction\", \"image_captioning\", \"refCOCOg\", \"ALL\"], label=\"Task\",  info=\"For now object classification/image captioning\", value=\"object_classification\")\n",
    "                    backbone = gr.Dropdown([\"llava-hf/llava-1.5-7b-hf\", \"meta-llama/Llama-2-7b-chat-hf\"], label=\"Decoder Backbone\",  info=\"Backbone Frozen LLM/VLM\", value=\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "                    \n",
    "                    freeze_llm = gr.Checkbox(label=\"freeze llm\", info=\"Freeze llm weights\", value=True)\n",
    "                    obj_encoder_checkpoint = gr.Checkbox(label=\"obj_encoder\", info=\"Use object encoder checkpoint\")\n",
    "                    use_retrieval = gr.Checkbox(label=\"use retrieval\", info=\"Use retrieval to understand prediction\")\n",
    "\n",
    "\n",
    "\n",
    "                    \n",
    "\n",
    "        #btn = gr.Button()\n",
    "        \n",
    "        im.change(sleep, outputs=[gallery], inputs=im) \n",
    "\n",
    "        \n",
    "            \n",
    "\n",
    "        #with gr.Column():    \n",
    "            # image_output = gr.AnnotatedImage(width=600, height=600)\n",
    "            # output_text = gr.Textbox()\n",
    "            # gr.Examples(\n",
    "            #     examples=[[os.path.join(\"example_pictures\", \"cat_and_dog.png\"), \"Image Captioning\", 32, True, True],\n",
    "            #               [os.path.join(\"example_pictures\", \"cat_and_dog.png\"), \"Object Classification\", 5, True, True],\n",
    "            #               [os.path.join(\"example_pictures\", \"birds.png\"), \"Object Classification\", 5, True, True]],\n",
    "            #     inputs=[im_out_1, task, n_layers, obj_encoder_checkpoint, freeze_llm],\n",
    "            #     outputs=[image_output, output_text],\n",
    "            #     fn=generate_predictions,\n",
    "            #     cache_examples=False,\n",
    "            # )\n",
    "            # text_output1 = gr.HighlightedText(\n",
    "            #                     label=\"Generated Description\", \n",
    "            #                     combine_adjacent=False,\n",
    "            #                     show_legend=True,\n",
    "            #                 ).style(color_map={\"box\": \"red\"})\n",
    "\n",
    "\n",
    "    submit_btn.click(fn=generate_predictions, \n",
    "                        inputs=[textbox, gallery, task, backbone, use_retrieval, obj_encoder_checkpoint, freeze_llm, chatbot],  \n",
    "                        outputs=[chatbot, retrieval_gallery],  \n",
    "                        show_progress=True, queue=True)\n",
    "\n",
    "demo.launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a098ab80",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mimage\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image' is not defined"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.09s/it]\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the .\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu/disk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized model with meta-llama/Llama-2-7b-chat-hf LLM backbone and openai/clip-vit-large-patch14 Vision Encoder\n",
      "It has 26064896 trainable parameters\n",
      "The save path is: ./checkpoints/llama_2_finetuned_checkpoints/object_classification/frozen_llm_clip_retrieval_16x16_patches\n",
      "Loaded Object Encoder checkpoint from ./checkpoints/llama_2_finetuned_checkpoints/object_classification/frozen_llm_clip_retrieval_16x16_patches\n",
      "164 393 189 434\n",
      "[[0.58, 0.57, 0.57, 0.57, 0.57]]\n",
      "torch.Size([1, 106, 4096]) torch.Size([1, 106]) torch.Size([1, 106])\n",
      "75 393 321 587\n",
      "[[0.68, 0.68, 0.67, 0.67, 0.67]]\n",
      "torch.Size([1, 109, 4096]) torch.Size([1, 109]) torch.Size([1, 109])\n",
      "75 393 321 590\n",
      "[[0.71, 0.71, 0.71, 0.71, 0.7]]\n",
      "torch.Size([1, 106, 4096]) torch.Size([1, 106]) torch.Size([1, 106])\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
