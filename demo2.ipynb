{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b40d341b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/ossowski/anaconda3/envs/llava/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'n_patches'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m         config \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39msafe_load(file)\n\u001b[1;32m      8\u001b[0m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_retrieval\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m dataset \u001b[38;5;241m=\u001b[39m COCOObjectDataset(config, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, n_patches\u001b[38;5;241m=\u001b[39m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mn_patches\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, max_examples_per_class \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexamples_per_class\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     11\u001b[0m old_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'n_patches'"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from models.visionllama import VisionLLaMA\n",
    "from dataset.objectCOCO import COCOObjectDataset\n",
    "\n",
    "\n",
    "with open(\"configs/config.yaml\", 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "\n",
    "config['n_patches'] = 16\n",
    "if \"336\" in config['vision_encoder']:\n",
    "config['n_patches'] = 24\n",
    "        \n",
    "config['use_retrieval'] = True\n",
    "dataset = COCOObjectDataset(config, split=\"train\", n_patches=config['n_patches'], max_examples_per_class = config[\"examples_per_class\"])\n",
    "model = None\n",
    "old_config = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc98f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/work/ossowski/anaconda3/envs/llava/lib/python3.10/site-packages/gradio/queueing.py\", line 522, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/work/ossowski/anaconda3/envs/llava/lib/python3.10/site-packages/gradio/route_utils.py\", line 260, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/work/ossowski/anaconda3/envs/llava/lib/python3.10/site-packages/gradio/blocks.py\", line 1689, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/work/ossowski/anaconda3/envs/llava/lib/python3.10/site-packages/gradio/blocks.py\", line 1255, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/ua/ossowski/.local/lib/python3.10/site-packages/anyio/to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/ua/ossowski/.local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/ua/ossowski/.local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/work/ossowski/anaconda3/envs/llava/lib/python3.10/site-packages/gradio/utils.py\", line 750, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_2827682/2398021567.py\", line 57, in generate_predictions\n",
      "    image = Image.open(images[0][0]).convert('RGB')\n",
      "TypeError: 'NoneType' object is not subscriptable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('/tmp/gradio/ed72398b3d6752e9c0fd4a098bd59b07df7134ac/image.png', None), ('/tmp/gradio/38c8b5c0119a88b95cc9b6bec238ce0456f7140c/image.png', None)]\n",
      "24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/ossowski/anaconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:08<00:00,  2.99s/it]\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the .\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu/disk.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized model with llava-hf/llava-1.5-7b-hf LLM backbone and openai/clip-vit-large-patch14-336 Vision Encoder\n",
      "The save path is: ./checkpoints/llava_finetuned_checkpoints/ObjectInstruct/finetuned_llm_clip_336_24x24_patches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/work/ossowski/anaconda3/envs/llava/lib/python3.10/site-packages/gradio/queueing.py\", line 522, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/work/ossowski/anaconda3/envs/llava/lib/python3.10/site-packages/gradio/route_utils.py\", line 260, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/work/ossowski/anaconda3/envs/llava/lib/python3.10/site-packages/gradio/blocks.py\", line 1689, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/work/ossowski/anaconda3/envs/llava/lib/python3.10/site-packages/gradio/blocks.py\", line 1255, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/ua/ossowski/.local/lib/python3.10/site-packages/anyio/to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/ua/ossowski/.local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/ua/ossowski/.local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/work/ossowski/anaconda3/envs/llava/lib/python3.10/site-packages/gradio/utils.py\", line 750, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_2827682/2398021567.py\", line 90, in generate_predictions\n",
      "    model.load()\n",
      "  File \"/work/ossowski/alignment/models/visionllama.py\", line 518, in load\n",
      "    self.object_encoder.load_state_dict(torch.load(os.path.join(SAVE_PATH, \"llama_2_7b_adapter_finetuned\")))\n",
      "  File \"/work/ossowski/anaconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 2152, in load_state_dict\n",
      "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
      "RuntimeError: Error(s) in loading state_dict for ObjectEncoder:\n",
      "\tMissing key(s) in state_dict: \"model.vision_model.encoder.layers.0.self_attn.k_proj.weight\", \"model.vision_model.encoder.layers.0.self_attn.k_proj.bias\", \"model.vision_model.encoder.layers.0.self_attn.v_proj.weight\", \"model.vision_model.encoder.layers.0.self_attn.v_proj.bias\", \"model.vision_model.encoder.layers.0.self_attn.q_proj.weight\", \"model.vision_model.encoder.layers.0.self_attn.q_proj.bias\", \"model.vision_model.encoder.layers.1.self_attn.k_proj.weight\", \"model.vision_model.encoder.layers.1.self_attn.k_proj.bias\", \"model.vision_model.encoder.layers.1.self_attn.v_proj.weight\", \"model.vision_model.encoder.layers.1.self_attn.v_proj.bias\", \"model.vision_model.encoder.layers.1.self_attn.q_proj.weight\", \"model.vision_model.encoder.layers.1.self_attn.q_proj.bias\", \"model.vision_model.encoder.layers.2.self_attn.k_proj.weight\", \"model.vision_model.encoder.layers.2.self_attn.k_proj.bias\", \"model.vision_model.encoder.layers.2.self_attn.v_proj.weight\", \"model.vision_model.encoder.layers.2.self_attn.v_proj.bias\", \"model.vision_model.encoder.layers.2.self_attn.q_proj.weight\", \"model.vision_model.encoder.layers.2.self_attn.q_proj.bias\", \"model.vision_model.encoder.layers.3.self_attn.k_proj.weight\", \"model.vision_model.encoder.layers.3.self_attn.k_proj.bias\", \"model.vision_model.encoder.layers.3.self_attn.v_proj.weight\", \"model.vision_model.encoder.layers.3.self_attn.v_proj.bias\", \"model.vision_model.encoder.layers.3.self_attn.q_proj.weight\", \"model.vision_model.encoder.layers.3.self_attn.q_proj.bias\", \"model.vision_model.encoder.layers.4.self_attn.k_proj.weight\", \"model.vision_model.encoder.layers.4.self_attn.k_proj.bias\", \"model.vision_model.encoder.layers.4.self_attn.v_proj.weight\", \"model.vision_model.encoder.layers.4.self_attn.v_proj.bias\", \"model.vision_model.encoder.layers.4.self_attn.q_proj.weight\", \"model.vision_model.encoder.layers.4.self_attn.q_proj.bias\", \"model.vision_model.encoder.layers.5.self_attn.k_proj.weight\", \"model.vision_model.encoder.layers.5.self_attn.k_proj.bias\", \"model.vision_model.encoder.layers.5.self_attn.v_proj.weight\", \"model.vision_model.encoder.layers.5.self_attn.v_proj.bias\", \"model.vision_model.encoder.layers.5.self_attn.q_proj.weight\", \"model.vision_model.encoder.layers.5.self_attn.q_proj.bias\", \"model.vision_model.encoder.layers.6.self_attn.k_proj.weight\", \"model.vision_model.encoder.layers.6.self_attn.k_proj.bias\", \"model.vision_model.encoder.layers.6.self_attn.v_proj.weight\", \"model.vision_model.encoder.layers.6.self_attn.v_proj.bias\", \"model.vision_model.encoder.layers.6.self_attn.q_proj.weight\", \"model.vision_model.encoder.layers.6.self_attn.q_proj.bias\", \"model.vision_model.encoder.layers.7.self_attn.k_proj.weight\", \"model.vision_model.encoder.layers.7.self_attn.k_proj.bias\", \"model.vision_model.encoder.layers.7.self_attn.v_proj.weight\", \"model.vision_model.encoder.layers.7.self_attn.v_proj.bias\", \"model.vision_model.encoder.layers.7.self_attn.q_proj.weight\", \"model.vision_model.encoder.layers.7.self_attn.q_proj.bias\", \"model.vision_model.encoder.layers.8.self_attn.k_proj.weight\", \"model.vision_model.encoder.layers.8.self_attn.k_proj.bias\", \"model.vision_model.encoder.layers.8.self_attn.v_proj.weight\", \"model.vision_model.encoder.layers.8.self_attn.v_proj.bias\", \"model.vision_model.encoder.layers.8.self_attn.q_proj.weight\", \"model.vision_model.encoder.layers.8.self_attn.q_proj.bias\", \"model.vision_model.encoder.layers.9.self_attn.k_proj.weight\", \"model.vision_model.encoder.layers.9.self_attn.k_proj.bias\", \"model.vision_model.encoder.layers.9.self_attn.v_proj.weight\", \"model.vision_model.encoder.layers.9.self_attn.v_proj.bias\", \"model.vision_model.encoder.layers.9.self_attn.q_proj.weight\", \"model.vision_model.encoder.layers.9.self_attn.q_proj.bias\", \"model.vision_model.encoder.layers.10.self_attn.k_proj.weight\", \"model.vision_model.encoder.layers.10.self_attn.k_proj.bias\", \"model.vision_model.encoder.layers.10.self_attn.v_proj.weight\", \"model.vision_model.encoder.layers.10.self_attn.v_proj.bias\", \"model.vision_model.encoder.layers.10.self_attn.q_proj.weight\", \"model.vision_model.encoder.layers.10.self_attn.q_proj.bias\", \"model.vision_model.encoder.layers.11.self_attn.k_proj.weight\", \"model.vision_model.encoder.layers.11.self_attn.k_proj.bias\", \"model.vision_model.encoder.layers.11.self_attn.v_proj.weight\", \"model.vision_model.encoder.layers.11.self_attn.v_proj.bias\", \"model.vision_model.encoder.layers.11.self_attn.q_proj.weight\", \"model.vision_model.encoder.layers.11.self_attn.q_proj.bias\", \"model.vision_model.encoder.layers.12.self_attn.k_proj.weight\", \"model.vision_model.encoder.layers.12.self_attn.k_proj.bias\", \"model.vision_model.encoder.layers.12.self_attn.v_proj.weight\", \"model.vision_model.encoder.layers.12.self_attn.v_proj.bias\", \"model.vision_model.encoder.layers.12.self_attn.q_proj.weight\", \"model.vision_model.encoder.layers.12.self_attn.q_proj.bias\", \"model.vision_model.encoder.layers.13.self_attn.k_proj.weight\", \"model.vision_model.encoder.layers.13.self_attn.k_proj.bias\", \"model.vision_model.encoder.layers.13.self_attn.v_proj.weight\", \"model.vision_model.encoder.layers.13.self_attn.v_proj.bias\", \"model.vision_model.encoder.layers.13.self_attn.q_proj.weight\", \"model.vision_model.encoder.layers.13.self_attn.q_proj.bias\", \"model.vision_model.encoder.layers.14.self_attn.k_proj.weight\", \"model.vision_model.encoder.layers.14.self_attn.k_proj.bias\", \"model.vision_model.encoder.layers.14.self_attn.v_proj.weight\", \"model.vision_model.encoder.layers.14.self_attn.v_proj.bias\", \"model.vision_model.encoder.layers.14.self_attn.q_proj.weight\", \"model.vision_model.encoder.layers.14.self_attn.q_proj.bias\", \"model.vision_model.encoder.layers.15.self_attn.k_proj.weight\", \"model.vision_model.encoder.layers.15.self_attn.k_proj.bias\", \"model.vision_model.encoder.layers.15.self_attn.v_proj.weight\", \"model.vision_model.encoder.layers.15.self_attn.v_proj.bias\", \"model.vision_model.encoder.layers.15.self_attn.q_proj.weight\", \"model.vision_model.encoder.layers.15.self_attn.q_proj.bias\", \"model.vision_model.encoder.layers.16.self_attn.k_proj.weight\", \"model.vision_model.encoder.layers.16.self_attn.k_proj.bias\", \"model.vision_model.encoder.layers.16.self_attn.v_proj.weight\", \"model.vision_model.encoder.layers.16.self_attn.v_proj.bias\", \"model.vision_model.encoder.layers.16.self_attn.q_proj.weight\", \"model.vision_model.encoder.layers.16.self_attn.q_proj.bias\", \"model.vision_model.encoder.layers.17.self_attn.k_proj.weight\", \"model.vision_model.encoder.layers.17.self_attn.k_proj.bias\", \"model.vision_model.encoder.layers.17.self_attn.v_proj.weight\", \"model.vision_model.encoder.layers.17.self_attn.v_proj.bias\", \"model.vision_model.encoder.layers.17.self_attn.q_proj.weight\", \"model.vision_model.encoder.layers.17.self_attn.q_proj.bias\", \"model.vision_model.encoder.layers.18.self_attn.k_proj.weight\", \"model.vision_model.encoder.layers.18.self_attn.k_proj.bias\", \"model.vision_model.encoder.layers.18.self_attn.v_proj.weight\", \"model.vision_model.encoder.layers.18.self_attn.v_proj.bias\", \"model.vision_model.encoder.layers.18.self_attn.q_proj.weight\", \"model.vision_model.encoder.layers.18.self_attn.q_proj.bias\", \"model.vision_model.encoder.layers.19.self_attn.k_proj.weight\", \"model.vision_model.encoder.layers.19.self_attn.k_proj.bias\", \"model.vision_model.encoder.layers.19.self_attn.v_proj.weight\", \"model.vision_model.encoder.layers.19.self_attn.v_proj.bias\", \"model.vision_model.encoder.layers.19.self_attn.q_proj.weight\", \"model.vision_model.encoder.layers.19.self_attn.q_proj.bias\", \"model.vision_model.encoder.layers.20.self_attn.k_proj.weight\", \"model.vision_model.encoder.layers.20.self_attn.k_proj.bias\", \"model.vision_model.encoder.layers.20.self_attn.v_proj.weight\", \"model.vision_model.encoder.layers.20.self_attn.v_proj.bias\", \"model.vision_model.encoder.layers.20.self_attn.q_proj.weight\", \"model.vision_model.encoder.layers.20.self_attn.q_proj.bias\", \"model.vision_model.encoder.layers.21.self_attn.k_proj.weight\", \"model.vision_model.encoder.layers.21.self_attn.k_proj.bias\", \"model.vision_model.encoder.layers.21.self_attn.v_proj.weight\", \"model.vision_model.encoder.layers.21.self_attn.v_proj.bias\", \"model.vision_model.encoder.layers.21.self_attn.q_proj.weight\", \"model.vision_model.encoder.layers.21.self_attn.q_proj.bias\", \"model.vision_model.encoder.layers.22.self_attn.k_proj.weight\", \"model.vision_model.encoder.layers.22.self_attn.k_proj.bias\", \"model.vision_model.encoder.layers.22.self_attn.v_proj.weight\", \"model.vision_model.encoder.layers.22.self_attn.v_proj.bias\", \"model.vision_model.encoder.layers.22.self_attn.q_proj.weight\", \"model.vision_model.encoder.layers.22.self_attn.q_proj.bias\", \"model.vision_model.encoder.layers.23.self_attn.k_proj.weight\", \"model.vision_model.encoder.layers.23.self_attn.k_proj.bias\", \"model.vision_model.encoder.layers.23.self_attn.v_proj.weight\", \"model.vision_model.encoder.layers.23.self_attn.v_proj.bias\", \"model.vision_model.encoder.layers.23.self_attn.q_proj.weight\", \"model.vision_model.encoder.layers.23.self_attn.q_proj.bias\". \n",
      "\tUnexpected key(s) in state_dict: \"model.vision_model.encoder.layers.0.self_attn.k_proj.base_layer.weight\", \"model.vision_model.encoder.layers.0.self_attn.k_proj.base_layer.bias\", \"model.vision_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.0.self_attn.v_proj.base_layer.weight\", \"model.vision_model.encoder.layers.0.self_attn.v_proj.base_layer.bias\", \"model.vision_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.0.self_attn.q_proj.base_layer.weight\", \"model.vision_model.encoder.layers.0.self_attn.q_proj.base_layer.bias\", \"model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.1.self_attn.k_proj.base_layer.weight\", \"model.vision_model.encoder.layers.1.self_attn.k_proj.base_layer.bias\", \"model.vision_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.1.self_attn.v_proj.base_layer.weight\", \"model.vision_model.encoder.layers.1.self_attn.v_proj.base_layer.bias\", \"model.vision_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.1.self_attn.q_proj.base_layer.weight\", \"model.vision_model.encoder.layers.1.self_attn.q_proj.base_layer.bias\", \"model.vision_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.2.self_attn.k_proj.base_layer.weight\", \"model.vision_model.encoder.layers.2.self_attn.k_proj.base_layer.bias\", \"model.vision_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.2.self_attn.v_proj.base_layer.weight\", \"model.vision_model.encoder.layers.2.self_attn.v_proj.base_layer.bias\", \"model.vision_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.2.self_attn.q_proj.base_layer.weight\", \"model.vision_model.encoder.layers.2.self_attn.q_proj.base_layer.bias\", \"model.vision_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.3.self_attn.k_proj.base_layer.weight\", \"model.vision_model.encoder.layers.3.self_attn.k_proj.base_layer.bias\", \"model.vision_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.3.self_attn.v_proj.base_layer.weight\", \"model.vision_model.encoder.layers.3.self_attn.v_proj.base_layer.bias\", \"model.vision_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.3.self_attn.q_proj.base_layer.weight\", \"model.vision_model.encoder.layers.3.self_attn.q_proj.base_layer.bias\", \"model.vision_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.4.self_attn.k_proj.base_layer.weight\", \"model.vision_model.encoder.layers.4.self_attn.k_proj.base_layer.bias\", \"model.vision_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.4.self_attn.v_proj.base_layer.weight\", \"model.vision_model.encoder.layers.4.self_attn.v_proj.base_layer.bias\", \"model.vision_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.4.self_attn.q_proj.base_layer.weight\", \"model.vision_model.encoder.layers.4.self_attn.q_proj.base_layer.bias\", \"model.vision_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.5.self_attn.k_proj.base_layer.weight\", \"model.vision_model.encoder.layers.5.self_attn.k_proj.base_layer.bias\", \"model.vision_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.5.self_attn.v_proj.base_layer.weight\", \"model.vision_model.encoder.layers.5.self_attn.v_proj.base_layer.bias\", \"model.vision_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.5.self_attn.q_proj.base_layer.weight\", \"model.vision_model.encoder.layers.5.self_attn.q_proj.base_layer.bias\", \"model.vision_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.6.self_attn.k_proj.base_layer.weight\", \"model.vision_model.encoder.layers.6.self_attn.k_proj.base_layer.bias\", \"model.vision_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.6.self_attn.v_proj.base_layer.weight\", \"model.vision_model.encoder.layers.6.self_attn.v_proj.base_layer.bias\", \"model.vision_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.6.self_attn.q_proj.base_layer.weight\", \"model.vision_model.encoder.layers.6.self_attn.q_proj.base_layer.bias\", \"model.vision_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.7.self_attn.k_proj.base_layer.weight\", \"model.vision_model.encoder.layers.7.self_attn.k_proj.base_layer.bias\", \"model.vision_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.7.self_attn.v_proj.base_layer.weight\", \"model.vision_model.encoder.layers.7.self_attn.v_proj.base_layer.bias\", \"model.vision_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.7.self_attn.q_proj.base_layer.weight\", \"model.vision_model.encoder.layers.7.self_attn.q_proj.base_layer.bias\", \"model.vision_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.8.self_attn.k_proj.base_layer.weight\", \"model.vision_model.encoder.layers.8.self_attn.k_proj.base_layer.bias\", \"model.vision_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.8.self_attn.v_proj.base_layer.weight\", \"model.vision_model.encoder.layers.8.self_attn.v_proj.base_layer.bias\", \"model.vision_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.8.self_attn.q_proj.base_layer.weight\", \"model.vision_model.encoder.layers.8.self_attn.q_proj.base_layer.bias\", \"model.vision_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.9.self_attn.k_proj.base_layer.weight\", \"model.vision_model.encoder.layers.9.self_attn.k_proj.base_layer.bias\", \"model.vision_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.9.self_attn.v_proj.base_layer.weight\", \"model.vision_model.encoder.layers.9.self_attn.v_proj.base_layer.bias\", \"model.vision_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.9.self_attn.q_proj.base_layer.weight\", \"model.vision_model.encoder.layers.9.self_attn.q_proj.base_layer.bias\", \"model.vision_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.10.self_attn.k_proj.base_layer.weight\", \"model.vision_model.encoder.layers.10.self_attn.k_proj.base_layer.bias\", \"model.vision_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.10.self_attn.v_proj.base_layer.weight\", \"model.vision_model.encoder.layers.10.self_attn.v_proj.base_layer.bias\", \"model.vision_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.10.self_attn.q_proj.base_layer.weight\", \"model.vision_model.encoder.layers.10.self_attn.q_proj.base_layer.bias\", \"model.vision_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.11.self_attn.k_proj.base_layer.weight\", \"model.vision_model.encoder.layers.11.self_attn.k_proj.base_layer.bias\", \"model.vision_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.11.self_attn.v_proj.base_layer.weight\", \"model.vision_model.encoder.layers.11.self_attn.v_proj.base_layer.bias\", \"model.vision_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.11.self_attn.q_proj.base_layer.weight\", \"model.vision_model.encoder.layers.11.self_attn.q_proj.base_layer.bias\", \"model.vision_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.12.self_attn.k_proj.base_layer.weight\", \"model.vision_model.encoder.layers.12.self_attn.k_proj.base_layer.bias\", \"model.vision_model.encoder.layers.12.self_attn.k_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.12.self_attn.k_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.12.self_attn.v_proj.base_layer.weight\", \"model.vision_model.encoder.layers.12.self_attn.v_proj.base_layer.bias\", \"model.vision_model.encoder.layers.12.self_attn.v_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.12.self_attn.v_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.12.self_attn.q_proj.base_layer.weight\", \"model.vision_model.encoder.layers.12.self_attn.q_proj.base_layer.bias\", \"model.vision_model.encoder.layers.12.self_attn.q_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.12.self_attn.q_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.13.self_attn.k_proj.base_layer.weight\", \"model.vision_model.encoder.layers.13.self_attn.k_proj.base_layer.bias\", \"model.vision_model.encoder.layers.13.self_attn.k_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.13.self_attn.k_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.13.self_attn.v_proj.base_layer.weight\", \"model.vision_model.encoder.layers.13.self_attn.v_proj.base_layer.bias\", \"model.vision_model.encoder.layers.13.self_attn.v_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.13.self_attn.v_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.13.self_attn.q_proj.base_layer.weight\", \"model.vision_model.encoder.layers.13.self_attn.q_proj.base_layer.bias\", \"model.vision_model.encoder.layers.13.self_attn.q_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.13.self_attn.q_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.14.self_attn.k_proj.base_layer.weight\", \"model.vision_model.encoder.layers.14.self_attn.k_proj.base_layer.bias\", \"model.vision_model.encoder.layers.14.self_attn.k_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.14.self_attn.k_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.14.self_attn.v_proj.base_layer.weight\", \"model.vision_model.encoder.layers.14.self_attn.v_proj.base_layer.bias\", \"model.vision_model.encoder.layers.14.self_attn.v_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.14.self_attn.v_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.14.self_attn.q_proj.base_layer.weight\", \"model.vision_model.encoder.layers.14.self_attn.q_proj.base_layer.bias\", \"model.vision_model.encoder.layers.14.self_attn.q_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.14.self_attn.q_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.15.self_attn.k_proj.base_layer.weight\", \"model.vision_model.encoder.layers.15.self_attn.k_proj.base_layer.bias\", \"model.vision_model.encoder.layers.15.self_attn.k_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.15.self_attn.k_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.15.self_attn.v_proj.base_layer.weight\", \"model.vision_model.encoder.layers.15.self_attn.v_proj.base_layer.bias\", \"model.vision_model.encoder.layers.15.self_attn.v_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.15.self_attn.v_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.15.self_attn.q_proj.base_layer.weight\", \"model.vision_model.encoder.layers.15.self_attn.q_proj.base_layer.bias\", \"model.vision_model.encoder.layers.15.self_attn.q_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.15.self_attn.q_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.16.self_attn.k_proj.base_layer.weight\", \"model.vision_model.encoder.layers.16.self_attn.k_proj.base_layer.bias\", \"model.vision_model.encoder.layers.16.self_attn.k_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.16.self_attn.k_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.16.self_attn.v_proj.base_layer.weight\", \"model.vision_model.encoder.layers.16.self_attn.v_proj.base_layer.bias\", \"model.vision_model.encoder.layers.16.self_attn.v_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.16.self_attn.v_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.16.self_attn.q_proj.base_layer.weight\", \"model.vision_model.encoder.layers.16.self_attn.q_proj.base_layer.bias\", \"model.vision_model.encoder.layers.16.self_attn.q_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.16.self_attn.q_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.17.self_attn.k_proj.base_layer.weight\", \"model.vision_model.encoder.layers.17.self_attn.k_proj.base_layer.bias\", \"model.vision_model.encoder.layers.17.self_attn.k_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.17.self_attn.k_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.17.self_attn.v_proj.base_layer.weight\", \"model.vision_model.encoder.layers.17.self_attn.v_proj.base_layer.bias\", \"model.vision_model.encoder.layers.17.self_attn.v_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.17.self_attn.v_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.17.self_attn.q_proj.base_layer.weight\", \"model.vision_model.encoder.layers.17.self_attn.q_proj.base_layer.bias\", \"model.vision_model.encoder.layers.17.self_attn.q_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.17.self_attn.q_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.18.self_attn.k_proj.base_layer.weight\", \"model.vision_model.encoder.layers.18.self_attn.k_proj.base_layer.bias\", \"model.vision_model.encoder.layers.18.self_attn.k_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.18.self_attn.k_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.18.self_attn.v_proj.base_layer.weight\", \"model.vision_model.encoder.layers.18.self_attn.v_proj.base_layer.bias\", \"model.vision_model.encoder.layers.18.self_attn.v_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.18.self_attn.v_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.18.self_attn.q_proj.base_layer.weight\", \"model.vision_model.encoder.layers.18.self_attn.q_proj.base_layer.bias\", \"model.vision_model.encoder.layers.18.self_attn.q_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.18.self_attn.q_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.19.self_attn.k_proj.base_layer.weight\", \"model.vision_model.encoder.layers.19.self_attn.k_proj.base_layer.bias\", \"model.vision_model.encoder.layers.19.self_attn.k_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.19.self_attn.k_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.19.self_attn.v_proj.base_layer.weight\", \"model.vision_model.encoder.layers.19.self_attn.v_proj.base_layer.bias\", \"model.vision_model.encoder.layers.19.self_attn.v_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.19.self_attn.v_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.19.self_attn.q_proj.base_layer.weight\", \"model.vision_model.encoder.layers.19.self_attn.q_proj.base_layer.bias\", \"model.vision_model.encoder.layers.19.self_attn.q_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.19.self_attn.q_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.20.self_attn.k_proj.base_layer.weight\", \"model.vision_model.encoder.layers.20.self_attn.k_proj.base_layer.bias\", \"model.vision_model.encoder.layers.20.self_attn.k_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.20.self_attn.k_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.20.self_attn.v_proj.base_layer.weight\", \"model.vision_model.encoder.layers.20.self_attn.v_proj.base_layer.bias\", \"model.vision_model.encoder.layers.20.self_attn.v_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.20.self_attn.v_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.20.self_attn.q_proj.base_layer.weight\", \"model.vision_model.encoder.layers.20.self_attn.q_proj.base_layer.bias\", \"model.vision_model.encoder.layers.20.self_attn.q_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.20.self_attn.q_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.21.self_attn.k_proj.base_layer.weight\", \"model.vision_model.encoder.layers.21.self_attn.k_proj.base_layer.bias\", \"model.vision_model.encoder.layers.21.self_attn.k_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.21.self_attn.k_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.21.self_attn.v_proj.base_layer.weight\", \"model.vision_model.encoder.layers.21.self_attn.v_proj.base_layer.bias\", \"model.vision_model.encoder.layers.21.self_attn.v_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.21.self_attn.v_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.21.self_attn.q_proj.base_layer.weight\", \"model.vision_model.encoder.layers.21.self_attn.q_proj.base_layer.bias\", \"model.vision_model.encoder.layers.21.self_attn.q_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.21.self_attn.q_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.22.self_attn.k_proj.base_layer.weight\", \"model.vision_model.encoder.layers.22.self_attn.k_proj.base_layer.bias\", \"model.vision_model.encoder.layers.22.self_attn.k_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.22.self_attn.k_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.22.self_attn.v_proj.base_layer.weight\", \"model.vision_model.encoder.layers.22.self_attn.v_proj.base_layer.bias\", \"model.vision_model.encoder.layers.22.self_attn.v_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.22.self_attn.v_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.22.self_attn.q_proj.base_layer.weight\", \"model.vision_model.encoder.layers.22.self_attn.q_proj.base_layer.bias\", \"model.vision_model.encoder.layers.22.self_attn.q_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.22.self_attn.q_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.23.self_attn.k_proj.base_layer.weight\", \"model.vision_model.encoder.layers.23.self_attn.k_proj.base_layer.bias\", \"model.vision_model.encoder.layers.23.self_attn.k_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.23.self_attn.k_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.23.self_attn.v_proj.base_layer.weight\", \"model.vision_model.encoder.layers.23.self_attn.v_proj.base_layer.bias\", \"model.vision_model.encoder.layers.23.self_attn.v_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.23.self_attn.v_proj.lora_B.default.weight\", \"model.vision_model.encoder.layers.23.self_attn.q_proj.base_layer.weight\", \"model.vision_model.encoder.layers.23.self_attn.q_proj.base_layer.bias\", \"model.vision_model.encoder.layers.23.self_attn.q_proj.lora_A.default.weight\", \"model.vision_model.encoder.layers.23.self_attn.q_proj.lora_B.default.weight\". \n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import os\n",
    "import skimage\n",
    "import math\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "def _get_ViT_mask(mask, height, width, output_height, output_width):\n",
    "        \n",
    "    \n",
    "    pooled_mask = skimage.measure.block_reduce(mask, block_size=(math.floor(height / output_height), math.floor(width / output_width)), func=np.max)\n",
    "\n",
    "    result_height, result_width = pooled_mask.shape\n",
    "    # If the result is smaller than 16x16, pad it with zeros\n",
    "    if result_height < output_height or result_width < output_width:\n",
    "        pad_height = output_height - result_height\n",
    "        pad_width = output_width - result_width\n",
    "        pooled_mask = np.pad(pooled_mask, ((0, pad_height), (0, pad_width)), mode='constant')\n",
    "\n",
    "    if result_height > output_height or result_width > output_width:\n",
    "        pooled_mask = pooled_mask[:output_height, :output_width]\n",
    "\n",
    "    assert pooled_mask.shape == (output_height,output_width)\n",
    "    return torch.BoolTensor(np.append(1, pooled_mask.flatten()))\n",
    "\n",
    "def sleep(im):\n",
    "    time.sleep(2)\n",
    "    ret = [im[\"background\"]]\n",
    "    for layer in im[\"layers\"]:\n",
    "        ret.append(layer)\n",
    "    return ret\n",
    "    #return [im[\"background\"], im[\"layers\"][0], im[\"layers\"][1], im['composite']]\n",
    "\n",
    "def show_anns(anns, ax):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
    "    ax.set_autoscale_on(False)\n",
    "\n",
    "    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n",
    "    img[:,:,3] = 0\n",
    "    for ann in sorted_anns:\n",
    "        m = ann['segmentation']\n",
    "        color_mask = np.concatenate([np.random.random(3), [0.35]])\n",
    "        img[m] = color_mask\n",
    "    ax.imshow(img)\n",
    "\n",
    "def generate_predictions(question, images, task, backbone, use_retrieval, use_object_encoder_checkpoint, freeze_llm, chat_history):\n",
    "    global model\n",
    "    global old_config\n",
    "    print(images)\n",
    "    image = Image.open(images[0][0]).convert('RGB')\n",
    "    segmentations = [Image.open(x[0]).convert('RGB') for x in images[1:]]\n",
    "    \n",
    "    config['freeze_llm'] = freeze_llm\n",
    "    config['llm_model'] = backbone\n",
    "    config['task'] = task\n",
    "\n",
    "    if not use_object_encoder_checkpoint:\n",
    "        config['pretrained_object_encoder_checkpoint'] = \"None\"\n",
    "    else:\n",
    "        config['pretrained_object_encoder_checkpoint'] = \"./llama_2_7b_adapter\"\n",
    "\n",
    "    config['use_retrieval'] = use_retrieval\n",
    "\n",
    "    if \"llama\" or \"gpt2\" in backbone:\n",
    "        if \"336\" in config[\"vision_encoder\"]:\n",
    "            output_width, output_height = 24, 24\n",
    "        else:\n",
    "            output_width, output_height = 16, 16\n",
    "        \n",
    "    elif \"llava\" in backbone:\n",
    "        output_width, output_height = 24, 24\n",
    "\n",
    "    config['n_patches'] = output_width\n",
    "    print(config['n_patches'])\n",
    "\n",
    "\n",
    "    if old_config != config:\n",
    "        if config['use_retrieval']:\n",
    "            model = VisionLLaMA(config, retrieval_fn = lambda x, y: dataset.retrieve_closest(x, config[\"retrieval_k\"], b_num = y))    \n",
    "\n",
    "        else:\n",
    "            model = VisionLLaMA(config)\n",
    "        model.load()\n",
    "        model.eval()\n",
    "        old_config = config.copy()\n",
    "\n",
    "    seg_width, seg_height = image.size\n",
    "    \n",
    "    vit_masks = []\n",
    "\n",
    "    \n",
    "    cropped_images = []\n",
    "    for segmentation in segmentations:\n",
    "        seg = np.array(segmentation)\n",
    "        if np.sum(seg, axis = None) == 0:\n",
    "            continue\n",
    "        else:\n",
    "        \n",
    "            mask = np.any(seg != [0, 0, 0], axis=-1)\n",
    "\n",
    "            if config[\"crop_image\"]:\n",
    "                # print(mask.shape)\n",
    "                # print(img.shape)\n",
    "                img = np.array(image)\n",
    "                img[~mask] = np.array([255,255,255])\n",
    "                \n",
    "                \n",
    "                # Find the indices of non-zero elements in the binary mask\n",
    "                non_zero_indices = np.where(mask)\n",
    "\n",
    "                # Get the minimum and maximum values along each axis\n",
    "                min_x, min_y = np.min(non_zero_indices[1]), np.min(non_zero_indices[0])\n",
    "                max_x, max_y = np.max(non_zero_indices[1]), np.max(non_zero_indices[0])\n",
    "\n",
    "                #img = img[min_x: max_x, min_y: max_y]\n",
    "                img = img[min_y: max_y, min_x: max_x]\n",
    "\n",
    "                cropped_image = Image.fromarray(np.uint8(img)).convert('RGB')\n",
    "                cropped_images.append(cropped_image)\n",
    "                print(min_x, min_y, max_x, max_y)\n",
    "            \n",
    "            vit_masks.append(_get_ViT_mask(mask, seg_height, seg_width, output_height, output_width))\n",
    "       \n",
    "       #print(_get_ViT_mask(mask, seg_height, seg_width).shape)\n",
    "    if len(vit_masks) > 0:\n",
    "        vit_masks = torch.stack(vit_masks, axis = 0)\n",
    "    imgs = [image] * len(vit_masks) if len(vit_masks) > 0 else [image]\n",
    " \n",
    "\n",
    "    prompts = None\n",
    "    masks = None\n",
    "    images = None\n",
    "    if config['use_retrieval']:\n",
    "        output, prompts, masks, images = model.generate(vit_masks, imgs, [question], return_retrieved_info=True, cropped_images = cropped_images)\n",
    "        print(prompts)\n",
    "        chat_history.append((question, output))\n",
    "        retrieval_images = [Image.open(images[0][x]) for x in range(len(images[0]))]\n",
    "        #retrieval_images += [cropped_image]\n",
    "        return chat_history, retrieval_images\n",
    "        \n",
    "    else:\n",
    "        output = model.generate(vit_masks, imgs, [question])\n",
    "        chat_history.append((question, output))\n",
    "        return chat_history, None\n",
    "    \n",
    "    #print(prompts, masks, images)\n",
    "    # fig, ax = plt.subplots()\n",
    "    # ax.imshow(image)\n",
    "    # show_anns(masks, ax)\n",
    "    # plt.axis('off')\n",
    "\n",
    "    # img_buf = io.BytesIO()\n",
    "    # plt.savefig(img_buf, format='png')\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "with gr.Blocks(title=\"Olive\", theme=gr.themes.Base()).queue() as demo:\n",
    "    \n",
    "    with gr.Row():\n",
    "        \n",
    "        with gr.Column():\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    \n",
    "                    im = gr.ImageEditor(\n",
    "                        type=\"pil\"\n",
    "                        \n",
    "                    )\n",
    "                    with gr.Row():\n",
    "                        gallery = gr.Gallery(\n",
    "                            label=\"Segmentations\", show_label=False, elem_id=\"gallery\"\n",
    "                        , columns=[3], rows=[1], object_fit=\"contain\", height=200)\n",
    "\n",
    "\n",
    "                        \n",
    "                        # im_out_1 = gr.Image(type=\"pil\")\n",
    "                        # im_out_2 = gr.Image(type=\"pil\")\n",
    "                        # im_out_3 = gr.Image(type=\"pil\")\n",
    "                        # im_out_4 = gr.Image(type=\"pil\")\n",
    "                with gr.Column():\n",
    "                    chatbot = gr.Chatbot(elem_id=\"chatbot\", label=\"OLIVE Chatbot\", height=300)\n",
    "                    with gr.Row():\n",
    "                        \n",
    "                        with gr.Column(scale=8):\n",
    "                            textbox = gr.Textbox(show_label=False, placeholder=\"Enter text and press ENTER\", container=False)\n",
    "                        with gr.Column(scale=1, min_width=50):\n",
    "                            submit_btn = gr.Button(value=\"Send\", variant=\"primary\")\n",
    "                    retrieval_gallery = gr.Gallery(\n",
    "                            label=\"Retrieved Images\", show_label=True, elem_id=\"gallery2\"\n",
    "                        , columns=[5], rows=[1], object_fit=\"contain\", height=100)\n",
    "   \n",
    "                    #task = gr.Dropdown([\"object_classification\", \"relation_prediction\", \"image_captioning\", \"refCOCOg\", \"GRIT\", \"OCR\", \"ALL\", \"PointQA\", \"ObjectInstruct\"], label=\"Task\",  info=\"For now object classification/image captioning\", value=\"object_classification\")\n",
    "                    task = gr.Dropdown([\"object_classification\", \"relation_prediction\", \"image_captioning\", \"refCOCOg\", \"GRIT\", \"OCR\", \"ALL\", \"PointQA\", \"ObjectInstruct\"], label=\"Task\",  info=\"For now object classification/image captioning\", value=\"object_classification\")\n",
    "                    \n",
    "                    backbone = gr.Dropdown([\"llava-hf/llava-1.5-7b-hf\", \"meta-llama/Llama-2-7b-chat-hf\", \"gpt2\"], label=\"Decoder Backbone\",  info=\"Backbone Frozen LLM/VLM\", value=\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "                    \n",
    "                    freeze_llm = gr.Checkbox(label=\"freeze llm\", info=\"Freeze llm weights\", value=True)\n",
    "                    obj_encoder_checkpoint = gr.Checkbox(label=\"obj_encoder\", info=\"Use object encoder checkpoint\")\n",
    "                    use_retrieval = gr.Checkbox(label=\"use retrieval\", info=\"Use retrieval to understand prediction\")\n",
    "\n",
    "\n",
    "\n",
    "                    \n",
    "\n",
    "        #btn = gr.Button()\n",
    "        \n",
    "        im.change(sleep, outputs=[gallery], inputs=im) \n",
    "\n",
    "        \n",
    "            \n",
    "\n",
    "        #with gr.Column():    \n",
    "            # image_output = gr.AnnotatedImage(width=600, height=600)\n",
    "            # output_text = gr.Textbox()\n",
    "            # gr.Examples(\n",
    "            #     examples=[[os.path.join(\"example_pictures\", \"cat_and_dog.png\"), \"Image Captioning\", 32, True, True],\n",
    "            #               [os.path.join(\"example_pictures\", \"cat_and_dog.png\"), \"Object Classification\", 5, True, True],\n",
    "            #               [os.path.join(\"example_pictures\", \"birds.png\"), \"Object Classification\", 5, True, True]],\n",
    "            #     inputs=[im_out_1, task, n_layers, obj_encoder_checkpoint, freeze_llm],\n",
    "            #     outputs=[image_output, output_text],\n",
    "            #     fn=generate_predictions,\n",
    "            #     cache_examples=False,\n",
    "            # )\n",
    "            # text_output1 = gr.HighlightedText(\n",
    "            #                     label=\"Generated Description\", \n",
    "            #                     combine_adjacent=False,\n",
    "            #                     show_legend=True,\n",
    "            #                 ).style(color_map={\"box\": \"red\"})\n",
    "\n",
    "\n",
    "    submit_btn.click(fn=generate_predictions, \n",
    "                        inputs=[textbox, gallery, task, backbone, use_retrieval, obj_encoder_checkpoint, freeze_llm, chatbot],  \n",
    "                        outputs=[chatbot, retrieval_gallery],  \n",
    "                        show_progress=True, queue=True)\n",
    "\n",
    "demo.launch(inbrowser=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
