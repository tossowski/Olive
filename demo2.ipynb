{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b40d341b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/ossowski/anaconda3/envs/llava/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from models.visionllama import VisionLLaMA\n",
    "from dataset.objectCOCO import COCOObjectDataset\n",
    "\n",
    "\n",
    "with open(\"configs/config.yaml\", 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "config['use_retrieval'] = False\n",
    "#dataset = COCOObjectDataset(config, split=\"train\", patch_size=config['patch_size'], max_examples_per_class = config[\"examples_per_class\"])\n",
    "model = None\n",
    "old_config = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fc98f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/ossowski/anaconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.48s/it]\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the .\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu/disk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized model with meta-llama/Llama-2-7b-chat-hf LLM backbone and openai/clip-vit-large-patch14-336 Vision Encoder\n",
      "The save path is: ./checkpoints/llama_2_finetuned_checkpoints/GRIT/finetuned_llm_clip_336_24x24_patches\n",
      "Loaded Object Encoder checkpoint from ./checkpoints/llama_2_finetuned_checkpoints/GRIT/finetuned_llm_clip_336_24x24_patches\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import os\n",
    "import skimage\n",
    "import math\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "def _get_ViT_mask(mask, height, width, output_height, output_width):\n",
    "        \n",
    "    \n",
    "    pooled_mask = skimage.measure.block_reduce(mask, block_size=(math.floor(height / output_height), math.floor(width / output_width)), func=np.max)\n",
    "\n",
    "    result_height, result_width = pooled_mask.shape\n",
    "    # If the result is smaller than 16x16, pad it with zeros\n",
    "    if result_height < output_height or result_width < output_width:\n",
    "        pad_height = output_height - result_height\n",
    "        pad_width = output_width - result_width\n",
    "        pooled_mask = np.pad(pooled_mask, ((0, pad_height), (0, pad_width)), mode='constant')\n",
    "\n",
    "    if result_height > output_height or result_width > output_width:\n",
    "        pooled_mask = pooled_mask[:output_height, :output_width]\n",
    "\n",
    "    assert pooled_mask.shape == (output_height,output_width)\n",
    "    return torch.BoolTensor(np.append(1, pooled_mask.flatten()))\n",
    "\n",
    "def sleep(im):\n",
    "    time.sleep(2)\n",
    "    ret = [im[\"background\"]]\n",
    "    for layer in im[\"layers\"]:\n",
    "        ret.append(layer)\n",
    "    return ret\n",
    "    #return [im[\"background\"], im[\"layers\"][0], im[\"layers\"][1], im['composite']]\n",
    "\n",
    "def show_anns(anns, ax):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
    "    ax.set_autoscale_on(False)\n",
    "\n",
    "    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n",
    "    img[:,:,3] = 0\n",
    "    for ann in sorted_anns:\n",
    "        m = ann['segmentation']\n",
    "        color_mask = np.concatenate([np.random.random(3), [0.35]])\n",
    "        img[m] = color_mask\n",
    "    ax.imshow(img)\n",
    "\n",
    "def generate_predictions(question, images, task, backbone, use_retrieval, use_object_encoder_checkpoint, freeze_llm, chat_history):\n",
    "    global model\n",
    "    global old_config\n",
    "    image = Image.open(images.root[0].image.path).convert('RGB')\n",
    "    segmentations = [Image.open(x.image.path).convert('RGB') for x in images.root[1:]]\n",
    "    \n",
    "    config['freeze_llm'] = freeze_llm\n",
    "    config['llm_model'] = backbone\n",
    "    config['task'] = task\n",
    "\n",
    "    if not use_object_encoder_checkpoint:\n",
    "        config['pretrained_object_encoder_checkpoint'] = \"None\"\n",
    "    else:\n",
    "        config['pretrained_object_encoder_checkpoint'] = \"./llama_2_7b_adapter\"\n",
    "\n",
    "    config['use_retrieval'] = use_retrieval\n",
    "\n",
    "    if \"llama\" or \"gpt2\" in backbone:\n",
    "        if \"336\" in config[\"vision_encoder\"]:\n",
    "            output_width, output_height = 24, 24\n",
    "        else:\n",
    "            output_width, output_height = 16, 16\n",
    "        \n",
    "    elif \"llava\" in backbone:\n",
    "        output_width, output_height = 24, 24\n",
    "\n",
    "    config['patch_size'] = output_width\n",
    "    print(config['patch_size'])\n",
    "\n",
    "\n",
    "    if old_config != config:\n",
    "        if config['use_retrieval']:\n",
    "            model = VisionLLaMA(config, retrieval_fn = lambda x, y: dataset.retrieve_closest(x, config[\"retrieval_k\"], b_num = y))    \n",
    "\n",
    "        else:\n",
    "            model = VisionLLaMA(config)\n",
    "        model.load()\n",
    "        model.eval()\n",
    "        old_config = config.copy()\n",
    "\n",
    "    seg_width, seg_height = image.size\n",
    "    \n",
    "    vit_masks = []\n",
    "\n",
    "    \n",
    "    cropped_images = []\n",
    "    for segmentation in segmentations:\n",
    "        seg = np.array(segmentation)\n",
    "        if np.sum(seg, axis = None) == 0:\n",
    "            continue\n",
    "        else:\n",
    "        \n",
    "            mask = np.any(seg != [0, 0, 0], axis=-1)\n",
    "\n",
    "            if config[\"crop_image\"]:\n",
    "                # print(mask.shape)\n",
    "                # print(img.shape)\n",
    "                img = np.array(image)\n",
    "                img[~mask] = np.array([255,255,255])\n",
    "                \n",
    "                \n",
    "                # Find the indices of non-zero elements in the binary mask\n",
    "                non_zero_indices = np.where(mask)\n",
    "\n",
    "                # Get the minimum and maximum values along each axis\n",
    "                min_x, min_y = np.min(non_zero_indices[1]), np.min(non_zero_indices[0])\n",
    "                max_x, max_y = np.max(non_zero_indices[1]), np.max(non_zero_indices[0])\n",
    "\n",
    "                #img = img[min_x: max_x, min_y: max_y]\n",
    "                img = img[min_y: max_y, min_x: max_x]\n",
    "\n",
    "                cropped_image = Image.fromarray(np.uint8(img)).convert('RGB')\n",
    "                cropped_images.append(cropped_image)\n",
    "                print(min_x, min_y, max_x, max_y)\n",
    "            \n",
    "            vit_masks.append(_get_ViT_mask(mask, seg_height, seg_width, output_height, output_width))\n",
    "       \n",
    "       #print(_get_ViT_mask(mask, seg_height, seg_width).shape)\n",
    "    if len(vit_masks) > 0:\n",
    "        vit_masks = torch.stack(vit_masks, axis = 0)\n",
    "    imgs = [image] * len(vit_masks) if len(vit_masks) > 0 else [image]\n",
    " \n",
    "\n",
    "    prompts = None\n",
    "    masks = None\n",
    "    images = None\n",
    "    if config['use_retrieval']:\n",
    "        output, prompts, masks, images = model.generate(vit_masks, imgs, [question], return_retrieved_info=True, cropped_images = cropped_images)\n",
    "        print(prompts)\n",
    "        chat_history.append((question, output))\n",
    "        retrieval_images = [Image.open(images[0][x]) for x in range(len(images[0]))]\n",
    "        #retrieval_images += [cropped_image]\n",
    "        return chat_history, retrieval_images\n",
    "        \n",
    "    else:\n",
    "        output = model.generate(vit_masks, imgs, [question])\n",
    "        chat_history.append((question, output))\n",
    "        return chat_history, None\n",
    "    \n",
    "    #print(prompts, masks, images)\n",
    "    # fig, ax = plt.subplots()\n",
    "    # ax.imshow(image)\n",
    "    # show_anns(masks, ax)\n",
    "    # plt.axis('off')\n",
    "\n",
    "    # img_buf = io.BytesIO()\n",
    "    # plt.savefig(img_buf, format='png')\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "with gr.Blocks(title=\"Olive\", theme=gr.themes.Base()).queue() as demo:\n",
    "    \n",
    "    with gr.Row():\n",
    "        \n",
    "        with gr.Column():\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    \n",
    "                    im = gr.ImageEditor(\n",
    "                        type=\"pil\"\n",
    "                        \n",
    "                    )\n",
    "                    with gr.Row():\n",
    "                        gallery = gr.Gallery(\n",
    "                            label=\"Segmentations\", show_label=False, elem_id=\"gallery\"\n",
    "                        , columns=[3], rows=[1], object_fit=\"contain\", height=200)\n",
    "\n",
    "\n",
    "                        \n",
    "                        # im_out_1 = gr.Image(type=\"pil\")\n",
    "                        # im_out_2 = gr.Image(type=\"pil\")\n",
    "                        # im_out_3 = gr.Image(type=\"pil\")\n",
    "                        # im_out_4 = gr.Image(type=\"pil\")\n",
    "                with gr.Column():\n",
    "                    chatbot = gr.Chatbot(elem_id=\"chatbot\", label=\"LLaVA Chatbot\", height=550)\n",
    "                    with gr.Row():\n",
    "                        \n",
    "                        with gr.Column(scale=8):\n",
    "                            textbox = gr.Textbox(show_label=False, placeholder=\"Enter text and press ENTER\", container=False)\n",
    "                        with gr.Column(scale=1, min_width=50):\n",
    "                            submit_btn = gr.Button(value=\"Send\", variant=\"primary\")\n",
    "                    retrieval_gallery = gr.Gallery(\n",
    "                            label=\"Segmentations\", show_label=False, elem_id=\"gallery\"\n",
    "                        , columns=[5], rows=[1], object_fit=\"contain\", height=\"auto\")\n",
    "                    task = gr.Dropdown([\"object_classification\", \"relation_prediction\", \"image_captioning\", \"refCOCOg\", \"GRIT\", \"OCR\", \"ALL\", \"PointQA\"], label=\"Task\",  info=\"For now object classification/image captioning\", value=\"object_classification\")\n",
    "                    backbone = gr.Dropdown([\"llava-hf/llava-1.5-7b-hf\", \"meta-llama/Llama-2-7b-chat-hf\", \"gpt2\"], label=\"Decoder Backbone\",  info=\"Backbone Frozen LLM/VLM\", value=\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "                    \n",
    "                    freeze_llm = gr.Checkbox(label=\"freeze llm\", info=\"Freeze llm weights\", value=True)\n",
    "                    obj_encoder_checkpoint = gr.Checkbox(label=\"obj_encoder\", info=\"Use object encoder checkpoint\")\n",
    "                    use_retrieval = gr.Checkbox(label=\"use retrieval\", info=\"Use retrieval to understand prediction\")\n",
    "\n",
    "\n",
    "\n",
    "                    \n",
    "\n",
    "        #btn = gr.Button()\n",
    "        \n",
    "        im.change(sleep, outputs=[gallery], inputs=im) \n",
    "\n",
    "        \n",
    "            \n",
    "\n",
    "        #with gr.Column():    \n",
    "            # image_output = gr.AnnotatedImage(width=600, height=600)\n",
    "            # output_text = gr.Textbox()\n",
    "            # gr.Examples(\n",
    "            #     examples=[[os.path.join(\"example_pictures\", \"cat_and_dog.png\"), \"Image Captioning\", 32, True, True],\n",
    "            #               [os.path.join(\"example_pictures\", \"cat_and_dog.png\"), \"Object Classification\", 5, True, True],\n",
    "            #               [os.path.join(\"example_pictures\", \"birds.png\"), \"Object Classification\", 5, True, True]],\n",
    "            #     inputs=[im_out_1, task, n_layers, obj_encoder_checkpoint, freeze_llm],\n",
    "            #     outputs=[image_output, output_text],\n",
    "            #     fn=generate_predictions,\n",
    "            #     cache_examples=False,\n",
    "            # )\n",
    "            # text_output1 = gr.HighlightedText(\n",
    "            #                     label=\"Generated Description\", \n",
    "            #                     combine_adjacent=False,\n",
    "            #                     show_legend=True,\n",
    "            #                 ).style(color_map={\"box\": \"red\"})\n",
    "\n",
    "\n",
    "    submit_btn.click(fn=generate_predictions, \n",
    "                        inputs=[textbox, gallery, task, backbone, use_retrieval, obj_encoder_checkpoint, freeze_llm, chatbot],  \n",
    "                        outputs=[chatbot, retrieval_gallery],  \n",
    "                        show_progress=True, queue=True)\n",
    "\n",
    "demo.launch(inbrowser=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
