{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b40d341b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/ossowski/anaconda3/envs/olive/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 118287/118287 [00:58<00:00, 2010.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped over 3895 bad segmentations (no pixels)\n",
      "Loaded 846054 examples from retrieval/object_classification/retrieval_set_1000000_clip-vit-large-patch14-336.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:00<00:00, 2548.74it/s]\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from models.olive import OLIVE\n",
    "from dataset.objectCOCO import COCOObjectDataset\n",
    "import gradio as gr\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "import math\n",
    "import torch\n",
    "from PIL import Image\n",
    "from dataset.CXR8 import CXR8Dataset\n",
    "\n",
    "with open(\"configs/config.yaml\", 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "\n",
    "config['n_patches'] = 16\n",
    "if \"336\" in config['vision_encoder']:\n",
    "        config['n_patches'] = 24\n",
    "        \n",
    "if config[\"use_retrieval\"]:\n",
    "        #dataset = CXR8Dataset(config, split=\"train\", n_patches=config[\"n_patches\"])\n",
    "        dataset = COCOObjectDataset(config, split=\"train\", n_patches=config['n_patches'], max_examples_per_class = 1000000)\n",
    "\n",
    "model = None\n",
    "old_config = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fc98f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/ossowski/anaconda3/envs/olive/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized model with meta-llama/Llama-2-7b-chat-hf LLM backbone and openai/clip-vit-large-patch14-336 Vision Encoder\n",
      "The save path is: ./checkpoints/llama_2_finetuned_checkpoints/ALL/finetuned_llm_clip_336_retrieval_24x24_patches\n",
      "Loaded Object Encoder checkpoint from ./checkpoints/llama_2_finetuned_checkpoints/ALL/finetuned_llm_clip_336_retrieval_24x24_patches\n",
      "['The top 5 related objects are:\\n[obj] Crocodile with confidence 0.5\\n[obj] Crocodile with confidence 0.49\\n[obj] Crocodile with confidence 0.48\\n[obj] Crocodile with confidence 0.48\\n[obj] Kangaroo with confidence 0.47\\n\\n']\n"
     ]
    }
   ],
   "source": [
    "def _get_ViT_mask(mask, height, width, output_height, output_width):\n",
    "    pooled_mask = skimage.measure.block_reduce(mask, block_size=(math.floor(height / output_height), math.floor(width / output_width)), func=np.max)\n",
    "\n",
    "    result_height, result_width = pooled_mask.shape\n",
    "    # If the result is smaller than 16x16, pad it with zeros\n",
    "    if result_height < output_height or result_width < output_width:\n",
    "        pad_height = output_height - result_height\n",
    "        pad_width = output_width - result_width\n",
    "        pooled_mask = np.pad(pooled_mask, ((0, pad_height), (0, pad_width)), mode='constant')\n",
    "\n",
    "    if result_height > output_height or result_width > output_width:\n",
    "        pooled_mask = pooled_mask[:output_height, :output_width]\n",
    "\n",
    "    assert pooled_mask.shape == (output_height,output_width)\n",
    "    return torch.BoolTensor(np.append(1, pooled_mask.flatten()))\n",
    "\n",
    "def sleep(im):\n",
    "    time.sleep(2)\n",
    "    ret = [im[\"background\"]]\n",
    "    for layer in im[\"layers\"]:\n",
    "        ret.append(layer)\n",
    "    return ret\n",
    "\n",
    "def generate_predictions(question, images, task, backbone, use_retrieval, freeze_llm, chat_history):\n",
    "    global model\n",
    "    global old_config\n",
    "\n",
    "    image = Image.open(images[0][0]).convert('RGB')\n",
    "    segmentations = [Image.open(x[0]).convert('RGB') for x in images[1:]]\n",
    "    \n",
    "    config['freeze_llm'] = freeze_llm\n",
    "    config['llm_model'] = backbone\n",
    "    config['task'] = task\n",
    "    config['use_retrieval'] = use_retrieval\n",
    "\n",
    "    if \"llama\" or \"gpt2\" in backbone:\n",
    "        if \"336\" in config[\"vision_encoder\"]:\n",
    "            output_width, output_height = 24, 24\n",
    "        else:\n",
    "            output_width, output_height = 16, 16\n",
    "        \n",
    "    elif \"llava\" in backbone:\n",
    "        output_width, output_height = 24, 24\n",
    "\n",
    "    config['n_patches'] = output_width\n",
    "\n",
    "    if old_config != config:\n",
    "        if config['use_retrieval']:\n",
    "            model = OLIVE(config, retrieval_fn = lambda x, y: dataset.retrieve_closest(x, config[\"retrieval_k\"], train_phase=False, b_num = y))    \n",
    "        else:\n",
    "            model = OLIVE(config)\n",
    "        model.load()\n",
    "        model.eval()\n",
    "        old_config = config.copy()\n",
    "\n",
    "    seg_width, seg_height = image.size\n",
    "    \n",
    "    vit_masks = []\n",
    "\n",
    "    \n",
    "    cropped_images = []\n",
    "    for segmentation in segmentations:\n",
    "        seg = np.array(segmentation)\n",
    "        if np.sum(seg, axis = None) == 0:\n",
    "            continue\n",
    "        else:\n",
    "        \n",
    "            mask = np.any(seg != [0, 0, 0], axis=-1)\n",
    "\n",
    "            if config[\"crop_image\"]:\n",
    "                img = np.array(image)\n",
    "                img[~mask] = np.array([255,255,255])\n",
    "                \n",
    "                # Find the indices of non-zero elements in the binary mask\n",
    "                non_zero_indices = np.where(mask)\n",
    "\n",
    "                # Get the minimum and maximum values along each axis\n",
    "                min_x, min_y = np.min(non_zero_indices[1]), np.min(non_zero_indices[0])\n",
    "                max_x, max_y = np.max(non_zero_indices[1]), np.max(non_zero_indices[0])\n",
    "\n",
    "                img = img[min_y: max_y, min_x: max_x]\n",
    "\n",
    "                cropped_image = Image.fromarray(np.uint8(img)).convert('RGB')\n",
    "                cropped_images.append(cropped_image)\n",
    "            \n",
    "            vit_masks.append(_get_ViT_mask(mask, seg_height, seg_width, output_height, output_width))\n",
    "       \n",
    "    if len(vit_masks) > 0:\n",
    "        vit_masks = torch.stack(vit_masks, axis = 0)\n",
    "    imgs = [image] * len(vit_masks) if len(vit_masks) > 0 else [image]\n",
    " \n",
    "\n",
    "    prompts = None\n",
    "    masks = None\n",
    "    images = None\n",
    "    if config['use_retrieval']:\n",
    "        output, prompts, masks, images = model.generate(vit_masks, imgs, [question], return_retrieved_info=True, cropped_images = cropped_images)\n",
    "        chat_history.append((question, output))\n",
    "        retrieval_images = [Image.open(images[0][x]) for x in range(len(images[0]))]\n",
    "        return chat_history, retrieval_images\n",
    "        \n",
    "    else:\n",
    "        output = model.generate(vit_masks, imgs, [question])\n",
    "        chat_history.append((question, output))\n",
    "        return chat_history, None\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "with gr.Blocks(title=\"Olive\", theme=gr.themes.Base()).queue() as demo:\n",
    "    \n",
    "    with gr.Row():\n",
    "        \n",
    "        with gr.Column():\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    \n",
    "                    im = gr.ImageEditor(\n",
    "                        type=\"pil\"\n",
    "                    )\n",
    "\n",
    "                    with gr.Row():\n",
    "                        gallery = gr.Gallery(\n",
    "                            label=\"Segmentations\", show_label=False, elem_id=\"gallery\"\n",
    "                        , columns=[3], rows=[1], object_fit=\"contain\", height=200)\n",
    "\n",
    "                with gr.Column():\n",
    "                    chatbot = gr.Chatbot(elem_id=\"chatbot\", label=\"OLIVE Chatbot\", height=300)\n",
    "                    with gr.Row():\n",
    "                        \n",
    "                        with gr.Column(scale=8):\n",
    "                            textbox = gr.Textbox(show_label=False, placeholder=\"Enter text and press ENTER\", container=False)\n",
    "                        with gr.Column(scale=1, min_width=50):\n",
    "                            submit_btn = gr.Button(value=\"Send\", variant=\"primary\")\n",
    "                    retrieval_gallery = gr.Gallery(\n",
    "                            label=\"Retrieved Images\", show_label=True, elem_id=\"gallery2\"\n",
    "                        , columns=[5], rows=[1], object_fit=\"contain\", height=100)\n",
    "   \n",
    "                    task = gr.Dropdown([\"object_classification\", \"refCOCOg\", \"ALL\"], label=\"Task\",  info=\"For now object classification/image captioning\", value=\"object_classification\")\n",
    "                    \n",
    "                    backbone = gr.Dropdown([\"llava-hf/llava-1.5-7b-hf\", \"meta-llama/Llama-2-7b-chat-hf\", \"gpt2\"], label=\"Decoder Backbone\",  info=\"Backbone Frozen LLM/VLM\", value=\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "                    \n",
    "                    freeze_llm = gr.Checkbox(label=\"freeze llm\", info=\"Freeze llm weights\", value=True)\n",
    "                    use_retrieval = gr.Checkbox(label=\"use retrieval\", info=\"Use retrieval to understand prediction\")\n",
    "\n",
    "\n",
    "        \n",
    "        im.change(sleep, outputs=[gallery], inputs=im) \n",
    "\n",
    "        \n",
    "\n",
    "    submit_btn.click(fn=generate_predictions, \n",
    "                        inputs=[textbox, gallery, task, backbone, use_retrieval, freeze_llm, chatbot],  \n",
    "                        outputs=[chatbot, retrieval_gallery],  \n",
    "                        show_progress=True, queue=True)\n",
    "\n",
    "demo.launch(inbrowser=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
