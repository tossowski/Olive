{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b40d341b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/ossowski/anaconda3/envs/llava/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 118287/118287 [01:00<00:00, 1966.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped over 3895 bad segmentations (no pixels)\n",
      "Loaded 846054 examples from retrieval/object_classification/retrieval_set_1000000_clip-vit-large-patch14-336.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:00<00:00, 1533.04it/s]\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from models.olive import OLIVE\n",
    "from dataset.objectCOCO import COCOObjectDataset\n",
    "from dataset.CXR8 import CXR8Dataset\n",
    "\n",
    "with open(\"configs/config.yaml\", 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "\n",
    "# with open(\"configs/medical_adaptation.yaml\", 'r') as file:\n",
    "#         config = yaml.safe_load(file)\n",
    "\n",
    "config['n_patches'] = 16\n",
    "if \"336\" in config['vision_encoder']:\n",
    "        config['n_patches'] = 24\n",
    "        \n",
    "config['use_retrieval'] = True\n",
    "#dataset = CXR8Dataset(config, split=\"train\", n_patches=config[\"n_patches\"])\n",
    "dataset = COCOObjectDataset(config, split=\"train\", n_patches=config['n_patches'], max_examples_per_class = config[\"examples_per_class\"])\n",
    "model = None\n",
    "old_config = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fc98f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "IMPORTANT: You are using gradio version 4.24.0, however version 4.29.0 is available, please upgrade.\n",
      "--------\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('/tmp/gradio/27827b33a7762a376bdf9aee2d395c7d2aa33e43/image.png', None), ('/tmp/gradio/8493ad410eb37f7943c8cf5e49c160fee5dade4a/image.png', None)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/ossowski/anaconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:11<00:00,  3.75s/it]\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the .\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu/disk.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized model with llava-hf/llava-1.5-7b-hf LLM backbone and openai/clip-vit-large-patch14-336 Vision Encoder\n",
      "The LLM is FROZEN\n",
      "The save path is: ./checkpoints/llava_finetuned_checkpoints/refCOCOg/frozen_llm_clip_336_24x24_patches\n",
      "Loaded Object Encoder checkpoint from ./checkpoints/llava_finetuned_checkpoints/refCOCOg/frozen_llm_clip_336_24x24_patches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/ossowski/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/work/ossowski/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:427: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('/tmp/gradio/27827b33a7762a376bdf9aee2d395c7d2aa33e43/image.png', None), ('/tmp/gradio/8493ad410eb37f7943c8cf5e49c160fee5dade4a/image.png', None)]\n",
      "[('/tmp/gradio/27827b33a7762a376bdf9aee2d395c7d2aa33e43/image.png', None), ('/tmp/gradio/8493ad410eb37f7943c8cf5e49c160fee5dade4a/image.png', None)]\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import os\n",
    "import skimage\n",
    "import math\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "def _get_ViT_mask(mask, height, width, output_height, output_width):\n",
    "        \n",
    "    \n",
    "    pooled_mask = skimage.measure.block_reduce(mask, block_size=(math.floor(height / output_height), math.floor(width / output_width)), func=np.max)\n",
    "\n",
    "    result_height, result_width = pooled_mask.shape\n",
    "    # If the result is smaller than 16x16, pad it with zeros\n",
    "    if result_height < output_height or result_width < output_width:\n",
    "        pad_height = output_height - result_height\n",
    "        pad_width = output_width - result_width\n",
    "        pooled_mask = np.pad(pooled_mask, ((0, pad_height), (0, pad_width)), mode='constant')\n",
    "\n",
    "    if result_height > output_height or result_width > output_width:\n",
    "        pooled_mask = pooled_mask[:output_height, :output_width]\n",
    "\n",
    "    assert pooled_mask.shape == (output_height,output_width)\n",
    "    return torch.BoolTensor(np.append(1, pooled_mask.flatten()))\n",
    "\n",
    "def sleep(im):\n",
    "    time.sleep(2)\n",
    "    ret = [im[\"background\"]]\n",
    "    for layer in im[\"layers\"]:\n",
    "        ret.append(layer)\n",
    "    return ret\n",
    "    #return [im[\"background\"], im[\"layers\"][0], im[\"layers\"][1], im['composite']]\n",
    "\n",
    "def show_anns(anns, ax):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
    "    ax.set_autoscale_on(False)\n",
    "\n",
    "    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n",
    "    img[:,:,3] = 0\n",
    "    for ann in sorted_anns:\n",
    "        m = ann['segmentation']\n",
    "        color_mask = np.concatenate([np.random.random(3), [0.35]])\n",
    "        img[m] = color_mask\n",
    "    ax.imshow(img)\n",
    "\n",
    "def generate_predictions(question, images, task, backbone, use_retrieval, use_object_encoder_checkpoint, freeze_llm, chat_history):\n",
    "    global model\n",
    "    global old_config\n",
    "\n",
    "    image = Image.open(images[0][0]).convert('RGB')\n",
    "    segmentations = [Image.open(x[0]).convert('RGB') for x in images[1:]]\n",
    "    \n",
    "    config['freeze_llm'] = freeze_llm\n",
    "    config['llm_model'] = backbone\n",
    "    config['task'] = task\n",
    "\n",
    "    if not use_object_encoder_checkpoint:\n",
    "        config['pretrained_object_encoder_checkpoint'] = \"None\"\n",
    "    else:\n",
    "        config['pretrained_object_encoder_checkpoint'] = \"./llama_2_7b_adapter\"\n",
    "\n",
    "    config['use_retrieval'] = use_retrieval\n",
    "\n",
    "    if \"llama\" or \"gpt2\" in backbone:\n",
    "        if \"336\" in config[\"vision_encoder\"]:\n",
    "            output_width, output_height = 24, 24\n",
    "        else:\n",
    "            output_width, output_height = 16, 16\n",
    "        \n",
    "    elif \"llava\" in backbone:\n",
    "        output_width, output_height = 24, 24\n",
    "\n",
    "    config['n_patches'] = output_width\n",
    "\n",
    "    if old_config != config:\n",
    "        if config['use_retrieval']:\n",
    "            model = OLIVE(config, retrieval_fn = lambda x, y: dataset.retrieve_closest(x, config[\"retrieval_k\"], train_phase=False, b_num = y))    \n",
    "\n",
    "        else:\n",
    "            model = OLIVE(config)\n",
    "        model.load()\n",
    "        model.eval()\n",
    "        old_config = config.copy()\n",
    "\n",
    "    seg_width, seg_height = image.size\n",
    "    \n",
    "    vit_masks = []\n",
    "\n",
    "    \n",
    "    cropped_images = []\n",
    "    for segmentation in segmentations:\n",
    "        seg = np.array(segmentation)\n",
    "        if np.sum(seg, axis = None) == 0:\n",
    "            continue\n",
    "        else:\n",
    "        \n",
    "            mask = np.any(seg != [0, 0, 0], axis=-1)\n",
    "\n",
    "            if config[\"crop_image\"]:\n",
    "                # print(mask.shape)\n",
    "                # print(img.shape)\n",
    "                img = np.array(image)\n",
    "                img[~mask] = np.array([255,255,255])\n",
    "                \n",
    "                \n",
    "                # Find the indices of non-zero elements in the binary mask\n",
    "                non_zero_indices = np.where(mask)\n",
    "\n",
    "                # Get the minimum and maximum values along each axis\n",
    "                min_x, min_y = np.min(non_zero_indices[1]), np.min(non_zero_indices[0])\n",
    "                max_x, max_y = np.max(non_zero_indices[1]), np.max(non_zero_indices[0])\n",
    "\n",
    "                #img = img[min_x: max_x, min_y: max_y]\n",
    "                img = img[min_y: max_y, min_x: max_x]\n",
    "\n",
    "                cropped_image = Image.fromarray(np.uint8(img)).convert('RGB')\n",
    "                cropped_images.append(cropped_image)\n",
    "            \n",
    "            vit_masks.append(_get_ViT_mask(mask, seg_height, seg_width, output_height, output_width))\n",
    "       \n",
    "    if len(vit_masks) > 0:\n",
    "        vit_masks = torch.stack(vit_masks, axis = 0)\n",
    "    imgs = [image] * len(vit_masks) if len(vit_masks) > 0 else [image]\n",
    " \n",
    "\n",
    "    prompts = None\n",
    "    masks = None\n",
    "    images = None\n",
    "    if config['use_retrieval']:\n",
    "        output, prompts, masks, images = model.generate(vit_masks, imgs, [question], return_retrieved_info=True, cropped_images = cropped_images)\n",
    "        chat_history.append((question, output))\n",
    "        retrieval_images = [Image.open(images[0][x]) for x in range(len(images[0]))]\n",
    "        return chat_history, retrieval_images\n",
    "        \n",
    "    else:\n",
    "        output = model.generate(vit_masks, imgs, [question])\n",
    "        chat_history.append((question, output))\n",
    "        return chat_history, None\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "with gr.Blocks(title=\"Olive\", theme=gr.themes.Base()).queue() as demo:\n",
    "    \n",
    "    with gr.Row():\n",
    "        \n",
    "        with gr.Column():\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    \n",
    "                    im = gr.ImageEditor(\n",
    "                        type=\"pil\"\n",
    "                    )\n",
    "\n",
    "                    with gr.Row():\n",
    "                        gallery = gr.Gallery(\n",
    "                            label=\"Segmentations\", show_label=False, elem_id=\"gallery\"\n",
    "                        , columns=[3], rows=[1], object_fit=\"contain\", height=200)\n",
    "\n",
    "                with gr.Column():\n",
    "                    chatbot = gr.Chatbot(elem_id=\"chatbot\", label=\"OLIVE Chatbot\", height=300)\n",
    "                    with gr.Row():\n",
    "                        \n",
    "                        with gr.Column(scale=8):\n",
    "                            textbox = gr.Textbox(show_label=False, placeholder=\"Enter text and press ENTER\", container=False)\n",
    "                        with gr.Column(scale=1, min_width=50):\n",
    "                            submit_btn = gr.Button(value=\"Send\", variant=\"primary\")\n",
    "                    retrieval_gallery = gr.Gallery(\n",
    "                            label=\"Retrieved Images\", show_label=True, elem_id=\"gallery2\"\n",
    "                        , columns=[5], rows=[1], object_fit=\"contain\", height=100)\n",
    "   \n",
    "                    task = gr.Dropdown([\"object_classification\", \"relation_prediction\", \"image_captioning\", \"refCOCOg\", \"GRIT\", \"OCR\", \"ALL\", \"PointQA\", \"ObjectInstruct\"], label=\"Task\",  info=\"For now object classification/image captioning\", value=\"object_classification\")\n",
    "                    \n",
    "                    backbone = gr.Dropdown([\"llava-hf/llava-1.5-7b-hf\", \"meta-llama/Llama-2-7b-chat-hf\", \"gpt2\"], label=\"Decoder Backbone\",  info=\"Backbone Frozen LLM/VLM\", value=\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "                    \n",
    "                    freeze_llm = gr.Checkbox(label=\"freeze llm\", info=\"Freeze llm weights\", value=True)\n",
    "                    obj_encoder_checkpoint = gr.Checkbox(label=\"obj_encoder\", info=\"Use object encoder checkpoint\")\n",
    "                    use_retrieval = gr.Checkbox(label=\"use retrieval\", info=\"Use retrieval to understand prediction\")\n",
    "\n",
    "\n",
    "        \n",
    "        im.change(sleep, outputs=[gallery], inputs=im) \n",
    "\n",
    "        \n",
    "\n",
    "    submit_btn.click(fn=generate_predictions, \n",
    "                        inputs=[textbox, gallery, task, backbone, use_retrieval, obj_encoder_checkpoint, freeze_llm, chatbot],  \n",
    "                        outputs=[chatbot, retrieval_gallery],  \n",
    "                        show_progress=True, queue=True)\n",
    "\n",
    "demo.launch(inbrowser=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
